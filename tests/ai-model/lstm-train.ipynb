{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Model Training for Next-Page Prediction\n",
        "\n",
        "This notebook trains an LSTM model to predict the next page a user will visit based on their clickstream history.\n",
        "\n",
        "## Model Architecture\n",
        "- **Sequence Length**: 20\n",
        "- **Embedding Size**: 64\n",
        "- **LSTM Units**: 128\n",
        "- **Batch Size**: 64\n",
        "- **Train/Validation Split**: 80/20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = './data'\n",
        "SEQUENCE_LENGTH = 20\n",
        "EMBEDDING_SIZE = 64\n",
        "LSTM_UNITS = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"Loading preprocessed data...\")\n",
        "X_train = np.load(f'{DATA_DIR}/X_train.npy')\n",
        "X_val = np.load(f'{DATA_DIR}/X_val.npy')\n",
        "y_train = np.load(f'{DATA_DIR}/y_train.npy')\n",
        "y_val = np.load(f'{DATA_DIR}/y_val.npy')\n",
        "\n",
        "# Load vocabulary\n",
        "with open(f'{DATA_DIR}/vocab.json', 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "num_classes = vocab_size - 2  # Exclude <PAD> and <UNK>\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Sequence length: {SEQUENCE_LENGTH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_lstm_model(sequence_length, vocab_size, embedding_size, lstm_units, num_classes):\n",
        "    \"\"\"\n",
        "    Create LSTM model for next-page prediction.\n",
        "    \n",
        "    Architecture:\n",
        "    - Embedding layer: Maps page indices to dense vectors\n",
        "    - LSTM layer: Processes sequences\n",
        "    - Dense layer: Output predictions\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        # Embedding layer\n",
        "        layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_size,\n",
        "            input_length=sequence_length,\n",
        "            name='embedding'\n",
        "        ),\n",
        "        \n",
        "        # LSTM layer\n",
        "        layers.LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=False,  # Only return final output\n",
        "            name='lstm'\n",
        "        ),\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        layers.Dropout(0.2, name='dropout'),\n",
        "        \n",
        "        # Dense output layer\n",
        "        layers.Dense(\n",
        "            num_classes,\n",
        "            activation='softmax',\n",
        "            name='output'\n",
        "        )\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_lstm_model(\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_size=EMBEDDING_SIZE,\n",
        "    lstm_units=LSTM_UNITS,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create callbacks\n",
        "callbacks = [\n",
        "    # Early stopping\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Model checkpoint\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='./models/lstm_model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Reduce learning rate on plateau\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # CSV logger\n",
        "    keras.callbacks.CSVLogger(\n",
        "        filename='./results/training_history.csv',\n",
        "        append=False\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "os.makedirs('./results', exist_ok=True)\n",
        "\n",
        "print(\"Callbacks configured.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Starting training at {datetime.now()}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"Training completed at {datetime.now()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "print(\"Evaluating model on validation set...\")\n",
        "val_loss, val_accuracy, val_top_k = model.evaluate(X_val, y_val, verbose=1)\n",
        "\n",
        "print(f\"\\nValidation Results:\")\n",
        "print(f\"  Loss: {val_loss:.4f}\")\n",
        "print(f\"  Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"  Top-K Accuracy: {val_top_k:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "model_path = './models/lstm_final_model.h5'\n",
        "model.save(model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Also save in SavedModel format for TensorFlow.js conversion\n",
        "saved_model_path = './models/lstm_saved_model'\n",
        "model.save(saved_model_path, save_format='tf')\n",
        "print(f\"Model saved in SavedModel format to {saved_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Plot Training History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history.history['loss'], label='Training Loss')\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Model Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Model Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Training history plot saved to ./results/training_history.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Summary and Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model configuration\n",
        "config = {\n",
        "    'sequence_length': SEQUENCE_LENGTH,\n",
        "    'embedding_size': EMBEDDING_SIZE,\n",
        "    'lstm_units': LSTM_UNITS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'vocab_size': vocab_size,\n",
        "    'num_classes': num_classes,\n",
        "    'final_val_loss': float(val_loss),\n",
        "    'final_val_accuracy': float(val_accuracy),\n",
        "    'training_samples': int(len(X_train)),\n",
        "    'validation_samples': int(len(X_val))\n",
        "}\n",
        "\n",
        "with open('./results/model_config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Model configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nConfiguration saved to ./results/model_config.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
